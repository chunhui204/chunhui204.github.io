# 疑问

1. ROI pooling只crop proposal对应的原图区域, 是否缺少上下文信息？如果crop 1.x倍的区域是否更有利于微调bbox？

2. 在proposal_target_layer中negative样本的选择是0.1<IOU<0.5，为什么要有>0.1? 以及在0.5附近的样本类别是否变化太快，对网络有迷惑性？

>0.1的目的是选取hard negative example,这部分样本更具迷惑性更难训练，用这些样本训练模型更好一些。这算是hard negative mining的方法，但是OHEM认为这种方法不是最优的，并去掉了0.1这个限制，认为他仅凭IOU却忽略掉了一些不常见的，背景复杂的ROI。

# 改进

通过各种方式来增加上下文信息辅助检测。例如通过空间RNN来引入上下文信息[80]，通过放大faster rcnn的候选框来获得上下文信息[81][82][83]，用dilation 卷积来获得上下文[84][85]，用global pooling来获取上下文信息[86],对每个候选框都加入全局的分类结果来获得上下文信息[87]。Dssd[88]通过反卷积来利用SSD框架同一位置上高层感受野更大的特征增强本层的特征，来加入上下文信息。
改进分类损失。Sheng Tang等人[85]提出要加入sink类来改善某些背景类容易错分的情况， Tsung-Yi Lin等人[89]提出了focal loss，来改善单阶段框架下，类别数目不匹配的情况。
训练方式和样本扩增。OHEM[90]通过在线困难样本挖掘训练了更有判别力分类分支网络。A-fast-rcnn[91]采用生成对抗式网络的训练形式，在线产生训练困难的有遮挡或形变的样本。SSD[68]采用了丰富的数据扩增，包括镜像、颜色畸变、尺度缩放和纵横比缩放，极大提高了检测性能。
增强特征。Hypernet[92]把从高层到底层的多特征融合，然后进行ROIpooling，获得了更高的精度，FPN[93]通过反卷积网络，构建了每层都有相同特征强度的特征金字塔，对多尺度的目标都可以很好地处理。Jiannan Li [94]提出用生成对抗式网络式的训练，将小目标通过ROIpooling得到的特征逼近大目标ROIPooling提出的特征。
改进proposal 产生方式。J Hosang [95]通过实验表明，proposal方法的recall是影响检测器的性能的决定因素之一。CRAFT[98]通过两级的模型来回归出更好的object proposal。
改进回归方式，Spyros Gidaris [97]提出窗口微调与多窗口投票。首先利用Fast R-CNN[64]系列框架中对窗口进行回归的这个过程，反复迭代，然后用所有窗口投票，决定最终的目标类别与位置。

作者：AAAAAAIIIIII
链接：https://www.jianshu.com/p/70711a18f5f7
来源：简书
简书著作权归作者所有，任何形式的转载都请联系作者获得授权并注明出处。
