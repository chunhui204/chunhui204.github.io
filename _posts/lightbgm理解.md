1. 直方图方式处理特征。

在xgboost中构建叶子节点时，对一个数值特征（xgb不能直接处理类别特征）要先对该特征的所有样本排序，然后寻找一个切分点使二分之后信息增益最大，也就是预排序操作。预排序耗时，在lgbm中使用直方图
对数值特征（不管连续还是离散）进行处理，根据特征范围将其放入一个一个bin里，间接实现了排序操作，寻找最优切分点时只需要在bins之间找就可以了。这样处理既实现了特征离散化，又省掉了预排序操作。
而且在计算叶子节点的直方图就等于父节点与兄弟节点的差值，从加速了运算。特征离散化后寻找的最有切分点只是粗略的估计，但是相当于引入正则，避免了过拟合。当bins数目设置过少时会加速模型计算
但是特征离散的不够精确。

2. GROSS（大梯度样本采样）

在计算最有切分点时，如果数据及样本过多即使离散化后仍然有很多切分点的候选值，作者通过公式证明，在计算loss后下采样梯度较大的样本来计算信息增益可以近似使用全部样本计算的结果。

3.对categlory特征的处理

对于类别特征需要外部作Label encoding处理，然后在categlory_features指定该特征，lgbm会自动寻找该特征的最有切分点。对于non-tree模型一般通过one-hot encoding处理类别特征，但在树模型中使用onehot编码使得
特征稀疏化，必须构建很深的树才能达到较好的精度。（不知道gdbt和xgboost如何处理类别特征），而且在其他的树模型不能直接处理类别特征。在lgbm中我的理解是和数值特征一样，直接进行直方图离散化，
label encoding编码后的数值直接作为bins的值，相对大小的区别也就没有影响了。

4.特征合并

对于在大多数维度上不同时为零的特征之间（我理解是不相关）可以进行合并。如A特征范围是[0,10], B特征[0,20],那么可以合并为一个新的特征[0,30]代替原来的，这样减少了内存占用，在使用到这两个特征的时候再分离出来
就可以了。

5.leaf-wise增长方式

与xgboost的depth-wise增长不同，同一层上很多节点的信息增益很小，这样多降低loss的贡献是很小的。lgbm在信息增益最大的叶子节点上一直向下分裂直到信息增益很小为止，经证明同样的叶子节点数目这种增长方式
深度更小，但是这种方式更容易过拟合，要通过max_leaf_num或max_depth_num来限制。
